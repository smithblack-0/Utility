

# perform imports

import numpy as np
import torch
from torch import nn

import math
import numbers

#perform library imports
from Utility.Torch import Glimpses
from Utility.Torch import Activation

### Head accommodation on the linear layer ###

class Linear(nn.Module):
    """

    A Linear layer allowing head-dependent linear processing of data from shape
    to shape.

    An instance is made by providing a list of head_shapes,
    an input_shape tuple, an output_shape tuple.

    This is then used to initialize a head dependent linear remap
    from input shape to output shape. That will then be accessed
    through the instance call

    It is expected that the input format will be in the form of

    [..., heads, input_shape]

    Returning something of format

    [..., heads, output_shape]


    Letting the head_shape parameter be none will disable it, resulting in broadcasting. Input
    shape, output shape, and head_shapes may all be just an integer, in which case it is
    assumed only a single dimension is involved.

    """

    def __init__(self, input_shape, output_shape, head_shapes=None):
        #Super call

        super().__init__()

        # Implicit conversion
        if head_shapes is None:
            head_shapes = []
        elif isinstance(head_shapes, int):
            head_shapes = [head_shapes]
        if isinstance(input_shape, int):
            input_shape = [input_shape]
        if isinstance(output_shape, int):
            output_shape = [output_shape]

        # Create preprocesser and postprocessor. These flatten, and unflatten, the
        # dimensions we care about

        self._preprocessor = lambda x: Glimpses.view(x, input_shape, np.prod(input_shape))
        self._postprocesser = lambda x: Glimpses.view(x, np.prod(output_shape), output_shape)

        # Create kernel and bias. These include head dimensions if provided.

        if head_shapes is not None:
            kernel_shape = [*head_shapes, np.prod(output_shape), np.prod(input_shape)]
            bias_shape = [*head_shapes, np.prod(output_shape)]
        else:
            kernel_shape = [np.prod(output_shape), np.prod(input_shape)]
            bias_shape = [np.prod(output_shape)]

        kernel = torch.zeros(kernel_shape, requires_grad=True)
        kernel = torch.nn.init.kaiming_uniform_(kernel, a=math.sqrt(5))

        bias = torch.zeros(bias_shape, requires_grad=True)
        bias = torch.nn.init.zeros_(bias)

        # Store

        self._kernel = nn.Parameter(kernel)
        self._bias = nn.Parameter(bias)

    def forward(self, tensor):

        # Flatten the relavent dimensions

        tensor = self._preprocessor(tensor)

        # Perform primary processing. Add an extra dimension on the end
        # of the input tensor to handle the matrix multiply, perform
        # matrix multiply then add bias

        tensor = tensor.unsqueeze(-1)
        tensor = self._kernel.matmul(tensor)
        tensor = tensor.squeeze(-1)
        tensor = tensor + self._bias

        # Restore the dimensions
        tensor = self._postprocesser(tensor)

        # Return
        return tensor

### Archival and other data retrieval






class Archivist(nn.Module):

    """
    Description:

    The purpose of this class is to act as a container for holding
    the duel_usage indexer and retrieval specialist classes, which
    work together on the encoder and decoder portion of a model respectively.

    --- attributes ---

    userspace:

    class Indexer: A class, performs the task of indexing an encoder to produce
        an archive
    class Archive: A class. The output of an indexer. Consists of an index, and
        a few functions for retrieval and storage.
    class Access: A class. Accepts a list of archives and allows retrieval
        and even storage of them.

    --- methods ---

    __init__ : builds an Indexer and an Archive
    """
    class Archive():
        """

        Description:

        This class stores archival information generated by an
        indexer, and also contains the logic needed to get probabilities
        and possibilities in a gradient efficient manner.

        --- attributes ----

        archive_length: the length of the record underlying the archive.
        index_length: the synthetic length of the underlying index.
        record_dim: the dimensionality of each record.
        index_dim: the dimensionality of each index. Queries must match this width.


        _record: The underlying stored record
        _indexes: The indexes for the record.

        --- methods ---

        importance: A method designed to rank how closely each item in the archive is
            corrolated to a query. Returns log logits
        retrieve: A method designed to fetch informatiom out of the archive. Expects a


        """
        @property
        def archive_length(self):
            """ Returns the archive width length"""
            return self._record.shape[-2]
        @property
        def record_dim(self):
            """ Returns the record dimension"""
            return self._record.shape[-1]
        @property
        def index_dim(self):
            """ Returns the index dimension"""
            return self._indexes[0].shape[-1]
        def importance(self, queries):
            """
            Take each query in queries, and use it to generate
            log probabilities by adding together the conditions.

            :param queries:
                A (..., query_number, index_dims) vector where
                index_dims is the dimension of the index.
            :return: Log logits. A (..., query_number, item_importance) tensor where
            item_importance matches the width of the underlying record.

            This tends towards negative infinity when unimportant, and
            positive values when important. Recommend sigmoid for
            rectification.
            """

            #Go and generate each sectional logprob. These will be combined
            #together
            sectional_logprob = []
            for index in self._indexes:

                #Develop the raw corrolation
                raw = queries.matmul(index.transpose())
                raw = self.activation(raw) + 1e-4 # Addition here prevents dead gradients.

                #Develop and store the log corrolation
                logprob = torch.log(raw)
                sectional_logprob.append(logprob)

            #Stitch all of the probabilities together, while generating the
            #appropriate corrolations

            output = sectional_logprob[0]
            for item in sectional_logprob[1:]:
                current = item.unsqueeze(-1) #An extra dimensionss for the new smaller entries
                new = item.unsqueeze(-2) #An extra dimension for the current portion
                revision = new+current #The broadcasted logs combined.
                output = torch.flatten(revision, -2, -1)

           #Return stitched
            return output
        def retrieve(self, batched_selection):
            """
            Retrieves

            :param selection: A arbitrary shaped batch of boolean masks
                each of length archive_length.
            :return:
                A collection of entries from the record with length equal to the
                set of true entries in batched_selection, and with
                unspecified data defined to be zero.

                (..., longest_length)
            """

            #This function operates by performing a sort, getting what indices should
            #be retained, then selecting from the record and discarding all else.

            #Create the sorting and maximum data. The sorting data will
            #have a sort performed on it, and the indices retained. The
            #maximum data will be used to exclude everything which is
            #not needed in this return.

            sort_data = torch.where(batched_selection, 1, 0)
            masked_data = torch.where(batched_selection.unsqueeze(-1), self._record, 0)

            max_true = sort_data.type(torch.int32)  # Now int 0's and 1s
            max_true = max_true.sum(dim=-1)  # Number of trues per batch item
            max_true = max_true.max()  # Maximum true. Period.

            #Perform the sort, gather the indices, and generate the final data.
            #The sort function is constrained to return in descending order,
            #and maintain order. Since it sorts off sort_data,

            compress, decompress = Glimpses.compress_decompress(-1)

            _, indices = torch.sort(sort_data, descending=True, stable=True) #Get indices
            indices = indices[..., :(max_true+1)] #Discard indices which will never access data.
            selection_indices = compress(indices) #Compress the indices into a flat plane
            selected_records = masked_data.index_select(selection_indices) #Perform an index select using the flat list
            final_records = decompress(selected_records) #Restore the original shape

            return final_records

        def __init__(self, record, indexes, indexes_spacings, VG=True):

            #Store parameters
            self._record = record
            self._indexes = indexes
            self._spacings = indexes_spacings

            #Store activation.
            if VG:
                self.activation = Activation.vl_relu
            else:
                self.activation = torch.nn.ReLU()
        def forward(self, query, index_training=False):
            """

            The lookup and query function.









    class Indexer(nn.Module):
        """
        Description:

        The purpose of this class is to store information which has passed
        through the encode part of an encoder-decoder into a format which
        enables easy and more important efficient lookup of information
        relative to a particular query. It also serves to severely
        decouple the encoder from the decoder.

        Using the incoming data stream it generates a sequence of indexes
        - summaries of sequences of values at various layers. It then uses
        this to build an archive, which will return interesting subsections
        from the archive upon call

        """
    def __init__(self, archive_list):
        pass







class Transformer(nn.Module):
    __permitted = (None, "lower", "upper")

    @property
    def mask(self):
        return self._mask

    @mask.setter
    def mask(self, value):
        assert value in self.__permitted, "mask cannot be set to this"
        self._mask = value

    def __init__(self, channel_dim, head_width, mask=None):

        """

        Accepted mask is "lower", "upper", or none

        """

        # Spin up torch
        super().__init__()

        # Create action generators
        QueryGen = LinearReshape(channel_dim, (head_width, channel_dim))
        KeyGen = LinearReshape(channel_dim, (head_width, channel_dim))
        ValGen = LinearReshape(channel_dim, (head_width, channel_dim))

        CollapseGen = LinearReshape((head_width, channel_dim), channel_dim)

        # Create actions. Note the swap is needed to get the head in front of the items.

        self._query = lambda x: QueryGen(x).swapdims(-2, -3)
        self._key = lambda x: KeyGen(x).swapdims(-2, -3)
        self._value = lambda x: ValGen(x).swapdims(-2, -3)
        self._dehead = lambda x: CollapseGen(x.transpose(-2, -3))

        self.mask = mask

    def forward(self, query, content, mask=None):
        # Create query, key, value

        query = self._query(query)
        key = self._key(content).swapdims(-1, -2)
        value = self._value(content)

        # Create focus matrix. Mask. Softmax.

        focus = query.matmul(key)
        focus_dims = focus.shape[-2:]
        if mask is None:
            # Runs only if not provided a mask.
            if self.mask == "lower":
                mask = torch.tril(torch.ones(focus_dims))
                focus = focus.masked_fill(mask == 0, -1e9)
            if self.mask == "upper":
                mask = torch.triu(torch.ones(focus_dims))
                focus = focus.masked_fill(mask == 0, -1e9)

        focus = F.softmax(focus, dim=-1)

        # Apply focus matrix to values. Then compact head

        output = focus.matmul(value)
        output = self._dehead(output)

        return output

