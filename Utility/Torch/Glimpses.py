"""

This is a module for the manipulation of tensors by means of lightweight memory views and 
minimal padding. It extends the native torch functions in ways that I find useful.

All items within this module are functions. They all accept a tensor and parameters, then
do something with it. They also tend to return views to allow efficient memory utilization.

The current functions available are.

view
local


"""
import torch
from typing import Union, Sequence, List, Tuple

@torch.jit.script
def view(tensor,
         input_shape: Union[torch.Tensor, List[int], int],
         output_shape: Union[torch.Tensor, List[int], int]) -> torch.Tensor:
    """
    This will, when passed an input shape and compatible output shape, assume that said shapes
    refer to the later dimensions in a tensor, as in broadcasting, and will perform a reshape from
    input shape to output shape while keeping all other dimensions exactly the same.


    ---- parameters ---

    :param tensor:
        The tensor to be modified.
    :param input_shape:
        The expected input shape. This can be a list/tuple of ints, or an int. It should represent the shape at the end
        of the input tensor's .shape which will be matched in the tensor input
    :param output_shape:
        The expected output shape. This can be a list/tuple of ints, or an int. It should represent the final shape one
        wishes the tensor to take. It also must be the case that the total size of the input and output shape must be the same.
    ---- Examples ----


    For tensors of shape:

    a = (5,2), b=(3, 4, 5,2), c=(30, 5,2),

    For input_shape = (5,2), output_shape=10, one has

    f(a, input_shape, output_shape) = shape(10)
    f(b, input_shape, output_shape) = shape(3, 4, 10)
    f(c, input_shape, output_shape) = shape(30, 10)


    """

    # Raw Type converison. The goal here is to end up with something solely
    # in terms of tensors.

    if torch.jit.isinstance(input_shape, int):
        input_shape = [input_shape]
    if torch.jit.isinstance(output_shape, int):
        output_shape = [output_shape]

    if torch.jit.isinstance(input_shape, List[int]):
        input_shape = torch.tensor(input_shape, dtype=torch.int64)
    if torch.jit.isinstance(output_shape, List[int]):
        output_shape = torch.tensor(output_shape, dtype=torch.int64)

    torch.jit.annotate(torch.Tensor, input_shape)
    torch.jit.annotate(torch.Tensor, output_shape)

    # Basic sanity testing
    assert input_shape.prod() == output_shape.prod()\
        , "Shapes incompatible: Input shape and output shape were not compatible: "

    #Perform view action.
    slice_length: int = len(input_shape)
    static_shape: torch.Tensor = torch.tensor(tensor.shape[:-slice_length], dtype=torch.int64)

    final_shape: torch.Tensor = torch.concat([static_shape, output_shape])
    final_shape: List[int] = final_shape.tolist()

    output: torch.Tensor = tensor.view(final_shape)
    return output

@torch.jit.script
def local(tensor: torch.Tensor,
          kernel_width: int,
          stride_rate: int,
          dilation_rate: int,):
    """

    Description:

    This is a function designed to extract a series of kernels generated by standard convolutional
    keyword conventions which could, by broadcasted application of weights, be used to actually perform
    a convolution. The name "local" is due to the fact that the kernels generated are inherently a
    somewhat local phenomenon.

    When calling this function, a series of kernels with shape determined by dilation_rate and kernel_width,
    and with number determined by stride_rate, will be generated along the last dimension of the input tensor.
    The output will be a tensor with an additional dimension on the end, with width equal to the size of
    the kernel, and the second-to-last dimension then indices these kernels.

    Note that the different between initial and final indexing dimensions is:
        compensation = (kernel_width - 1) * dilation_rate

    Padding by this much is guaranteed to prevent information loss.

    """

    # Input Validation

    assert kernel_width >= 1, "kernel_width should be greater than or equal to 1"
    assert stride_rate >= 1, "stride_rate should be greater than or equal to 1"
    assert dilation_rate >= 1, "dilation_rate should be greater than or equal to 1"

    # Construct shape. Take into account the kernel_width, dilation rate, and stride rate.

    # The kernel width, and dilation rate, together modifies how far off the end of the
    # data buffer a naive implimentation would go, in an additive manner. Striding, meanwhile
    # is a multiplictive factor

    compensation: int = (kernel_width - 1) * dilation_rate  # calculate dilation-kernel correction
    final_index_shape = tensor.shape[-1] - compensation  # apply
    assert final_index_shape > 0, "Configuration is not possible - final kernel length exceeds available tensors"
    final_index_shape = final_index_shape // stride_rate  # Perform striding correction.

    static_shape = torch.tensor(tensor.shape[:-1], dtype=torch.int64)
    dynamic_shape = torch.tensor((final_index_shape, kernel_width), dtype=torch.int64)
    final_shape = torch.concat([static_shape, dynamic_shape])

    # Construct the stride. The main worry here is to ensure that the dilation striding, and primary
    # striding, now occurs at the correct rate. This is done by taking the current one, multiplying,
    # and putting this in the appropriate location.

    input_stride = [tensor.stride(dim) for dim in range(tensor.dim())] #Workaround for bad typing on Tensor.stride

    static_stride = torch.tensor(input_stride[:-1], dtype=torch.int64)
    dynamic_stride = torch.tensor((stride_rate * input_stride[-1], dilation_rate * input_stride[-1]), dtype=torch.int64)
    final_stride = torch.concat([static_stride, dynamic_stride])

    # perform extraction. Return result

    final_shape: List[int] = final_shape.tolist()
    final_stride: List[int] = final_stride.tolist()
    return tensor.as_strided(final_shape, final_stride)


def block(tensor, number):
    """

    Descrption:

    The purpose of this function is to split up the tensor
    into number equally sized units as a view.

    Excess is simply discarded.

    :param tensor:
    :param blocks:
    :return:
    """
    pass
