"""

Pseudo Lookup trees are constructed out of an incoming memory,
text_stream setup processed by earlier layers. They are designed
to be of fixed shape, and to possess properties which allow
computationally efficient lookup of information. Many exist in parallel.

The of looking up information process works something like this.
Construct a tree and the backmask operations out of memory,
stream tensors. When queries are provided for lookup purposes,
begin by querying first item in tree. Then query second item,
but backmask first query and apply mask in proportion to relationships
in first item. Continue until mask no longer has good suggestions,
or all items have been lookedup.

Terminology:

Tree Level: A single tensor in a set of ones built from each other, with each subsequent tensor
    being smaller and being built by attention from the previous layer.
Score Tensor: The tensor which results from multiplying the query by the key, masking, and softmax.
Score_Op: The attention tensor used to multiply the key tensor, run through the dehead step. Used
    to create Backmasks
Backmask: A tensor which is generated during an attention step with different queries vs key, values.
    Importantly, generated by monitoring tree interconnections, from the original query.
Pseudotree: A concept in which the attention weights, or 'scores', are said to form a sort of tree which
    show how closely related an input tensor is connected to an output one, and thus to what degree
    it can be said to be a tree under it.
"""

from __future__ import annotations
import math
from typing import List, Tuple, Optional

import torch
from torch import nn
from torch.nn import functional as F

from Utility.Torch.Learnables import Layers


### ENCODING ####
class TreeEncodeLayer(nn.Module):
    """

    Description:

    A unit to generate a single new set of tree parameters
    from the incoming tensors. This includes creating
    a new Score_Op and a new Tree Level, then storing them
    in the tensor stream

    """
    @staticmethod
    def _scaled_dot_product_attention(
            q: torch.Tensor,
            k: torch.Tensor,
            v: torch.Tensor,
            attn_mask: Optional[torch.Tensor] = None,
            dropout_p: float = 0.0,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        r"""
        A modified version of torch's _scaled_dot_product_attention,
        utilizing the sigmoid function rather than the softmax function

        Computes scaled dot product attention on query, key and value tensors, using
        an optional attention mask if passed, and applying dropout if a probability
        greater than 0.0 is specified.
        Returns a tensor pair containing attended values and attention weights.
        Args:
            q, k, v: query, key and value tensors. See Shape section for shape details.
            attn_mask: optional tensor containing mask values to be added to calculated
                attention. May be 2D or 3D; see Shape section for details.
            dropout_p: dropout probability. If greater than 0.0, dropout is applied.
        Shape:
            - q: :math:`(B, Nt, E)` where B is batch size, Nt is the target sequence length,
                and E is embedding dimension.
            - key: :math:`(B, Ns, E)` where B is batch size, Ns is the source sequence length,
                and E is embedding dimension.
            - value: :math:`(B, Ns, E)` where B is batch size, Ns is the source sequence length,
                and E is embedding dimension.
            - attn_mask: either a 3D tensor of shape :math:`(B, Nt, Ns)` or a 2D tensor of
                shape :math:`(Nt, Ns)`.
            - Output: attention values have shape :math:`(B, Nt, E)`; attention weights
                have shape :math:`(B, Nt, Ns)`
        """
        B, Nt, E = q.shape
        q = q / math.sqrt(E)
        # (B, Nt, E) x (B, E, Ns) -> (B, Nt, Ns)
        if attn_mask is not None:
            attn = torch.baddbmm(attn_mask, q, k.transpose(-2, -1))
        else:
            attn = torch.bmm(q, k.transpose(-2, -1))

        attn = F.sigmoid(attn)
        if dropout_p > 0.0:
            attn = F.dropout(attn, p=dropout_p)
        # (B, Nt, Ns) x (B, Ns, E) -> (B, Nt, E)
        output = torch.bmm(attn, v)
        return output, attn
    def __init__(self,
                 d_stream: int,
                 d_memory: int,
                 d_final: int,
                 width: int,
                 heads: int,
                 dropout: float,
                 dtype: torch.dtype):

        assert d_final % heads == 0
        assert d_stream % heads == 0
        assert d_memory % heads == 0

        super(TreeEncodeLayer, self).__init__()

        #Create norms and memory conditioning

        norm = nn.LayerNorm(d_final)

        #Create seed starting layer and memory conditioning attention
        seed = torch.zeros([width, d_memory],dtype=dtype, requires_grad=True)
        nn.init.kaiming_uniform_(seed)
        seed = nn.Parameter(seed)


        #Create primary attention layers.

        query_projector = Layers.Linear(d_memory, [heads, d_final])
        key_projector = Layers.Linear(d_final, [heads, d_final])
        value_projector = Layers.Linear(d_final, [heads, d_final])
        collapse_projector = Layers.Linear([heads, d_final], d_final)

        #Store entries

        self._dropout = dropout
        self._seed = seed
        self._norm = norm

        self._query_project = query_projector
        self._key_project = key_projector
        self._value_project = value_projector
        self._collapse = collapse_projector

    def forward(self,
                tree_values: torch.Tensor):

        #Setup query, key, value to possess heads and such
        query = self._query_project(self._seed).transpose(-2, -3).contiguous()
        key = self._key_project(tree_values).transpose(-2, -3).contiguous()
        value = self._value_project(tree_values).transpose(-2, -3).contiguous()

        #Perform attention. Return result
        output, score = self._scaled_dot_product_attention(query, key, value, dropout_p=self._dropout)
        return output, score.sum(dim=-1)




class TreeEncode(nn.Module):
    """
    The complete tree encode unit. Applies the encoding process to
    create the complete pseudotree
    """

    @staticmethod
    def make_submodel(
                      d_model: int,
                      widths: List[int],
                      heads: int,
                      dropout: float,
                      dtype: torch.dtype
                      ):
        """
        Makes an instance using the given parameters, and reasonable
        default assumptions.

        :param d_final: The final embeddign dimension
        :param d_memory: The memory embedding dimension
        :param d_stream: The stream embedding dimesion
        :param widths: A list of the widths of the stream heights
        :param heads: The heads to use
        :param dropout: The dropout rate to use
        :param dtype: The dtype
        :return: A new TreeEncodeSubmodel
        """
        layers = []
        for _ in range(total_submodels):
            layer = TreeEncodeLayer(d_model, )
            submodels.append(submodel)
        return TreeEncode(submodels)
    def __init__(self, submodels: List[nn.Module]):

        super(TreeEncode, self).__init__()
        self._submodels = submodels

    def forward(self,
                text_streams: List[torch.Tensor],
                memories: List[torch.Tensor]):

        #Develop reference and tree representation.
        lookup_references: List[List[torch.Tensor]] = []
        tree_representations: List[List[torch.Tensor]] = []

        for submodel, text_stream, memory in zip(self._submodels, text_streams, memories):
            sublookup_reference, subtree_rep = submodel(text_stream, memory)
            lookup_references.append(sublookup_reference)
            tree_representations.append(subtree_rep)

        lookup_references = list(map(list, zip(*lookup_references)))
        tree_representations = list(map(list, zip(*tree_representations)))
        final_lookup_references = [torch.concat(item, dim=-1) for item in lookup_references]



        return lookup_references, tree_representations


class KnowledgeTensor():
    """
    A collector for knowledge from various locations. Stores information
    in a sparse manner. Forgets unused knowledge. Returns it's update
    when a lookup is performed.
    """
    def lookup(self,
               query: torch.Tensor,
               decay: bool = True)-> Tuple[KnowledgeTensor, torch.Tensor, torch.Tensor]:
        """

        :param query:
        :param decay:
        :returns: KnowledgeTensor, query_result, loss
        """

    def append(self,
               lookup_references: List[List[torch.Tensor]],
               access_references: List[List[torch.Tensor]]):



    def __init__(self,
                 layer_index: List[torch.Tensor]
                 layer_access: List[torch.Tensor]
                 layer_descend: List[torch.Tensor]

                 ):

        self.levels = levels
        self.


class Oracle(nn.Module):
    """

    An oracle lookup layer. This knows how to traverse the
    data source it is connected to.

    """

    def __init__(self,
                 regularization_strength: float,
                 d_lookup: int,
                 top_level_width: int):

        super().__init__()

    def forward(self,
                query: torch.Tensor,
                sparse):