"""

Pseudo Lookup trees are constructed out of an incoming memory,
text_stream setup processed by earlier layers. They are designed
to be of fixed shape, and to possess properties which allow
computationally efficient lookup of information. Many exist in parallel.

The of looking up information process works something like this.
Construct a tree and the backmask operations out of memory,
stream tensors. When queries are provided for lookup purposes,
begin by querying first item in tree. Then query second item,
but backmask first query and apply mask in proportion to relationships
in first item. Continue until mask no longer has good suggestions,
or all items have been lookedup.

Terminology:

Tree Level: A single tensor in a set of ones built from each other, with each subsequent tensor
    being smaller and being built by attention from the previous layer.
Score Tensor: The tensor which results from multiplying the query by the key, masking, and softmax.
Score_Op: The attention tensor used to multiply the key tensor, run through the dehead step. Used
    to create Backmasks
Backmask: A tensor which is generated during an attention step with different queries vs key, values.
    Importantly, generated by monitoring tree interconnections, from the original query.
Pseudotree: A concept in which the attention weights, or 'scores', are said to form a sort of tree which
    show how closely related an input tensor is connected to an output one, and thus to what degree
    it can be said to be a tree under it.
"""

import math
from typing import List, Tuple, Optional

import torch
from torch import nn
from torch.nn import functional as F

from Utility.Torch.Learnables import Layers


### ENCODING ####
class TreeEncodeLayer(nn.Module):
    """

    Description:

    A unit to generate a single new set of tree parameters
    from the incoming tensors. This includes creating
    a new Score_Op and a new Tree Level, then storing them
    in the tensor stream

    """
    @staticmethod
    def _scaled_dot_product_attention(
            q: torch.Tensor,
            k: torch.Tensor,
            v: torch.Tensor,
            attn_mask: Optional[torch.Tensor] = None,
            dropout_p: float = 0.0,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        r"""
        A modified version of torch's _scaled_dot_product_attention,
        utilizing the sigmoid function rather than the softmax function

        Computes scaled dot product attention on query, key and value tensors, using
        an optional attention mask if passed, and applying dropout if a probability
        greater than 0.0 is specified.
        Returns a tensor pair containing attended values and attention weights.
        Args:
            q, k, v: query, key and value tensors. See Shape section for shape details.
            attn_mask: optional tensor containing mask values to be added to calculated
                attention. May be 2D or 3D; see Shape section for details.
            dropout_p: dropout probability. If greater than 0.0, dropout is applied.
        Shape:
            - q: :math:`(B, Nt, E)` where B is batch size, Nt is the target sequence length,
                and E is embedding dimension.
            - key: :math:`(B, Ns, E)` where B is batch size, Ns is the source sequence length,
                and E is embedding dimension.
            - value: :math:`(B, Ns, E)` where B is batch size, Ns is the source sequence length,
                and E is embedding dimension.
            - attn_mask: either a 3D tensor of shape :math:`(B, Nt, Ns)` or a 2D tensor of
                shape :math:`(Nt, Ns)`.
            - Output: attention values have shape :math:`(B, Nt, E)`; attention weights
                have shape :math:`(B, Nt, Ns)`
        """
        B, Nt, E = q.shape
        q = q / math.sqrt(E)
        # (B, Nt, E) x (B, E, Ns) -> (B, Nt, Ns)
        if attn_mask is not None:
            attn = torch.baddbmm(attn_mask, q, k.transpose(-2, -1))
        else:
            attn = torch.bmm(q, k.transpose(-2, -1))

        attn = F.sigmoid(attn)
        if dropout_p > 0.0:
            attn = F.dropout(attn, p=dropout_p)
        # (B, Nt, Ns) x (B, Ns, E) -> (B, Nt, E)
        output = torch.bmm(attn, v)
        return output, attn
    def __init__(self,
                 d_stream: int,
                 d_memory: int,
                 d_final: int,
                 width: int,
                 heads: int,
                 dropout: float,
                 dtype: torch.dtype):

        assert d_final % heads == 0
        assert d_stream % heads == 0
        assert d_memory % heads == 0

        super(TreeEncodeLayer, self).__init__()

        #Create norms and memory conditioning

        norm = nn.LayerNorm(d_final)

        #Create seed starting layer and memory conditioning attention
        seed = torch.zeros([width, d_memory],dtype=dtype, requires_grad=True)
        nn.init.kaiming_uniform_(seed)
        seed = nn.Parameter(seed)


        #Create primary attention layers.

        query_projector = Layers.Linear(d_memory, [heads, d_final])
        key_projector = Layers.Linear(d_final, [heads, d_final])
        value_projector = Layers.Linear(d_final, [heads, d_final])
        collapse_projector = Layers.Linear([heads, d_final], d_final)

        memory_conditioning = nn.MultiheadAttention(d_memory, heads, dropout, kdim=d_memory, vdim=d_memory, batch_first=True)

        #Store entries

        self._dropout = dropout
        self._seed = seed
        self._norm = norm
        self._memory = memory_conditioning

        self._query_project = query_projector
        self._key_project = key_projector
        self._value_project = value_projector
        self._collapse = collapse_projector

    def forward(self,
                tree_values: torch.Tensor,
                memory: torch.Tensor):


        query_seed = self._memory(self._seed, memory, memory)

        #Setup query, key, value to possess heads and such
        query = self._query_project(query_seed).transpose(-2, -3).contiguous()
        key = self._key_project(tree_values).transpose(-2, -3).contiguous()
        value = self._value_project(tree_values).transpose(-2, -3).contiguous()

        #Perform attention. Return result
        output, score = self._scaled_dot_product_attention(query, key, value, dropout_p=self._dropout)
        return output, score.sum(dim=-1)



class TreeEncodeSubmodel(nn.Module):
    """
    A complete tree encoding submodel, capable of generating
    a utility tree. Accepts a stream, memory tensor pair.
    Generates an interrelated tree stack, along with the required
    interrelated tensors for backmasking.

    """
    @classmethod
    def make_submodel(cls,
                      d_final: int,
                      d_memory: int,
                      d_stream: int,
                      widths: List[int],
                      heads: int,
                      dropout: float,
                      dtype: torch.dtype
                      ):
        """
        Makes a submodel using the given parameters.

        :param d_final: The final embeddign dimension
        :param d_memory: The memory embedding dimension
        :param d_stream: The stream embedding dimesion
        :param widths: A list of the widths of the stream heights
        :param heads: The heads to use
        :param dropout: The dropout rate to use
        :param dtype: The dtype
        :return: A new TreeEncodeSubmodel
        """
        layers = []
        for width in widths:
            layers.append(TreeEncodeLayer(d_stream, d_memory, d_final, width, heads, dropout, dtype))
        return TreeEncodeSubmodel(d_final, widths, layers, dtype)

    def __init__(self,
                 d_final: int,
                 widths: List[int],
                 layers: List[nn.Module],
                 dtype: torch.dtype
                 ):

        super(TreeEncodeSubmodel, self).__init__()
        self._layers = nn.ModuleList(layers)
        self._widths = widths
        self._d_final = d_final
        self._dtype = dtype

    def forward(self,
                text_stream: torch.Tensor,
                memory: torch.Tensor,
                ):

        first_loop = True
        lookup_reference: torch.Tensor = torch.tensor(0)
        tree_representation: torch.Tensor = torch.tensor(0)
        tensor = text_stream
        for layer in self._layers:
            tensor, subop = layer(tensor, memory)

            if first_loop:
                lookup_reference = tensor
                tree_representation = subop
                first_loop = False
                continue

            tree_update = torch.matmul(subop.transpose(-1, -2), tree_representation)
            tree_representation = torch.concat([subop, tree_update], dim=-1)
            lookup_reference = torch.concat([tensor, lookup_reference], dim=-1)

        identity_shape = list(tree_representation.shape[:-3]) + [tree_representation.shape[-1], tree_representation.shape[-1]]
        top_level_representation = torch.eye(tree_representation.shape[-1])
        top_level_representation = torch.broadcast_to(top_level_representation, identity_shape)
        tree_representation = torch.concat([top_level_representation, tree_representation], dim=-1)

        return lookup_reference, tree_representation





class TreeEncode(nn.Module):
    """
    The complete tree encode unit. Applies the encoding process to
    create the complete pseudotree
    """

    @staticmethod
    def make_submodel(
                      total_submodels: int,
                      d_final: int,
                      d_memory: int,
                      d_stream: int,
                      widths: List[int],
                      heads: int,
                      dropout: float,
                      dtype: torch.dtype
                      ):
        """
        Makes an instance using the given parameters, and reasonable
        default assumptions.

        :param d_final: The final embeddign dimension
        :param d_memory: The memory embedding dimension
        :param d_stream: The stream embedding dimesion
        :param widths: A list of the widths of the stream heights
        :param heads: The heads to use
        :param dropout: The dropout rate to use
        :param dtype: The dtype
        :return: A new TreeEncodeSubmodel
        """
        submodels = []
        for _ in range(total_submodels):
            submodel = TreeEncodeSubmodel.make_submodel(d_final, d_memory, d_stream, widths, heads, dropout, dtype)
            submodels.append(submodel)
        return TreeEncode(submodels)
    def __init__(self, submodels: List[nn.Module]):

        super(TreeEncode, self).__init__()
        self._submodels = submodels

    def forward(self,
                text_streams: List[torch.Tensor],
                memories: List[torch.Tensor]):

        #Develop reference and tree representation.
        lookup_references: List[torch.Tensor] = []
        tree_representations: List[torch.Tensor] = []

        for submodel, text_stream, memory in zip(self._submodels, text_streams, memories):
            sublookup_reference, subtree_rep = submodel(text_stream, memory)
            lookup_references.append(sublookup_reference)
            tree_representations.append(subtree_rep)

        lookup_reference = torch.concat(lookup_references, dim=-2)
        tree_representation = torch.concat(tree_representations, dim=-1)

        return lookup_reference, tree_representation

class KnowledgeBase()


class Oracle(nn.Module):
    """

    An oracle lookup layer. This knows how to traverse the
    data source it is connected to.

    """

    def __init__(self,
                 regularization_strength: float,
                 d_lookup: int,
                 top_level_width: int):

        super().__init__()

    def forward(self,
                query: torch.Tensor,
                sparse):